2023-03-29 18:07:13 | INFO | __main__:train:29) - Using: cuda
2023-03-29 18:07:15 | INFO | __main__:train:53) - Model:
 nnGAN(
  (generator): Sequential(
    (0): Linear(in_features=75, out_features=128, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=128, out_features=784, bias=True)
    (4): Tanh()
  )
  (discriminator): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=128, out_features=1, bias=True)
    (4): Sigmoid()
  )
)
2023-03-29 18:07:15 | INFO | __main__:train:118) - Epoch: 001/010 | Batch 000/469 | Gen/Dis Loss: 0.6516/0.7081
2023-03-29 18:07:17 | INFO | __main__:train:118) - Epoch: 001/010 | Batch 100/469 | Gen/Dis Loss: 5.0422/0.0353
2023-03-29 18:07:19 | INFO | __main__:train:118) - Epoch: 001/010 | Batch 200/469 | Gen/Dis Loss: 1.7976/0.1096
2023-03-29 18:07:20 | INFO | __main__:train:118) - Epoch: 001/010 | Batch 300/469 | Gen/Dis Loss: 1.4743/0.1647
2023-03-29 18:07:22 | INFO | __main__:train:118) - Epoch: 001/010 | Batch 400/469 | Gen/Dis Loss: 2.2171/0.0878
2023-03-29 18:07:23 | INFO | __main__:train:130) - Time elapsed: 0.16 min
2023-03-29 18:07:23 | INFO | __main__:train:118) - Epoch: 002/010 | Batch 000/469 | Gen/Dis Loss: 1.6828/0.2436
2023-03-29 18:07:25 | INFO | __main__:train:118) - Epoch: 002/010 | Batch 100/469 | Gen/Dis Loss: 1.3736/0.2487
2023-03-29 18:07:27 | INFO | __main__:train:118) - Epoch: 002/010 | Batch 200/469 | Gen/Dis Loss: 1.9087/0.1826
2023-03-29 18:07:28 | INFO | __main__:train:118) - Epoch: 002/010 | Batch 300/469 | Gen/Dis Loss: 1.3411/0.3890
2023-03-29 18:07:30 | INFO | __main__:train:118) - Epoch: 002/010 | Batch 400/469 | Gen/Dis Loss: 0.8213/0.4490
2023-03-29 18:07:31 | INFO | __main__:train:130) - Time elapsed: 0.29 min
2023-03-29 18:07:31 | INFO | __main__:train:118) - Epoch: 003/010 | Batch 000/469 | Gen/Dis Loss: 0.8057/0.4878
2023-03-29 18:07:33 | INFO | __main__:train:118) - Epoch: 003/010 | Batch 100/469 | Gen/Dis Loss: 1.0651/0.4279
2023-03-29 18:07:34 | INFO | __main__:train:118) - Epoch: 003/010 | Batch 200/469 | Gen/Dis Loss: 1.7373/0.3155
2023-03-29 18:07:36 | INFO | __main__:train:118) - Epoch: 003/010 | Batch 300/469 | Gen/Dis Loss: 1.0100/0.4667
2023-03-29 18:07:38 | INFO | __main__:train:118) - Epoch: 003/010 | Batch 400/469 | Gen/Dis Loss: 1.0844/0.4173
2023-03-29 18:07:39 | INFO | __main__:train:130) - Time elapsed: 0.42 min
2023-03-29 18:07:39 | INFO | __main__:train:118) - Epoch: 004/010 | Batch 000/469 | Gen/Dis Loss: 1.1890/0.4701
2023-03-29 18:07:41 | INFO | __main__:train:118) - Epoch: 004/010 | Batch 100/469 | Gen/Dis Loss: 0.9714/0.4926
2023-03-29 18:07:42 | INFO | __main__:train:118) - Epoch: 004/010 | Batch 200/469 | Gen/Dis Loss: 1.1019/0.4454
2023-03-29 18:07:44 | INFO | __main__:train:118) - Epoch: 004/010 | Batch 300/469 | Gen/Dis Loss: 1.2837/0.4539
2023-03-29 18:07:46 | INFO | __main__:train:118) - Epoch: 004/010 | Batch 400/469 | Gen/Dis Loss: 0.6943/0.6154
2023-03-29 18:07:47 | INFO | __main__:train:130) - Time elapsed: 0.56 min
2023-03-29 18:07:47 | INFO | __main__:train:118) - Epoch: 005/010 | Batch 000/469 | Gen/Dis Loss: 0.8201/0.5674
2023-03-29 18:07:49 | INFO | __main__:train:118) - Epoch: 005/010 | Batch 100/469 | Gen/Dis Loss: 0.7374/0.6214
2023-03-29 18:07:50 | INFO | __main__:train:118) - Epoch: 005/010 | Batch 200/469 | Gen/Dis Loss: 0.8702/0.5338
2023-03-29 18:07:52 | INFO | __main__:train:118) - Epoch: 005/010 | Batch 300/469 | Gen/Dis Loss: 1.4419/0.4626
2023-03-29 18:07:54 | INFO | __main__:train:118) - Epoch: 005/010 | Batch 400/469 | Gen/Dis Loss: 1.1724/0.5475
2023-03-29 18:07:55 | INFO | __main__:train:130) - Time elapsed: 0.69 min
2023-03-29 18:07:55 | INFO | __main__:train:118) - Epoch: 006/010 | Batch 000/469 | Gen/Dis Loss: 0.9596/0.5166
2023-03-29 18:07:57 | INFO | __main__:train:118) - Epoch: 006/010 | Batch 100/469 | Gen/Dis Loss: 0.7051/0.6235
2023-03-29 18:07:59 | INFO | __main__:train:118) - Epoch: 006/010 | Batch 200/469 | Gen/Dis Loss: 0.8157/0.5618
2023-03-29 18:08:00 | INFO | __main__:train:118) - Epoch: 006/010 | Batch 300/469 | Gen/Dis Loss: 0.7366/0.6014
2023-03-29 18:08:02 | INFO | __main__:train:118) - Epoch: 006/010 | Batch 400/469 | Gen/Dis Loss: 0.8309/0.5743
2023-03-29 18:08:03 | INFO | __main__:train:130) - Time elapsed: 0.83 min
2023-03-29 18:08:03 | INFO | __main__:train:118) - Epoch: 007/010 | Batch 000/469 | Gen/Dis Loss: 0.7915/0.5981
2023-03-29 18:08:05 | INFO | __main__:train:118) - Epoch: 007/010 | Batch 100/469 | Gen/Dis Loss: 0.9207/0.6576
2023-03-29 18:08:07 | INFO | __main__:train:118) - Epoch: 007/010 | Batch 200/469 | Gen/Dis Loss: 1.0874/0.5552
2023-03-29 18:08:08 | INFO | __main__:train:118) - Epoch: 007/010 | Batch 300/469 | Gen/Dis Loss: 0.8769/0.5484
2023-03-29 18:08:10 | INFO | __main__:train:118) - Epoch: 007/010 | Batch 400/469 | Gen/Dis Loss: 0.8998/0.5696
2023-03-29 18:08:11 | INFO | __main__:train:130) - Time elapsed: 0.96 min
2023-03-29 18:08:11 | INFO | __main__:train:118) - Epoch: 008/010 | Batch 000/469 | Gen/Dis Loss: 0.7868/0.6160
2023-03-29 18:08:13 | INFO | __main__:train:118) - Epoch: 008/010 | Batch 100/469 | Gen/Dis Loss: 0.8915/0.4829
2023-03-29 18:08:14 | INFO | __main__:train:118) - Epoch: 008/010 | Batch 200/469 | Gen/Dis Loss: 1.2184/0.5254
2023-03-29 18:08:16 | INFO | __main__:train:118) - Epoch: 008/010 | Batch 300/469 | Gen/Dis Loss: 1.1782/0.4361
2023-03-29 18:08:18 | INFO | __main__:train:118) - Epoch: 008/010 | Batch 400/469 | Gen/Dis Loss: 0.9872/0.4729
2023-03-29 18:08:19 | INFO | __main__:train:130) - Time elapsed: 1.09 min
2023-03-29 18:08:19 | INFO | __main__:train:118) - Epoch: 009/010 | Batch 000/469 | Gen/Dis Loss: 1.5149/0.5351
2023-03-29 18:08:21 | INFO | __main__:train:118) - Epoch: 009/010 | Batch 100/469 | Gen/Dis Loss: 1.4595/0.4654
2023-03-29 18:08:23 | INFO | __main__:train:118) - Epoch: 009/010 | Batch 200/469 | Gen/Dis Loss: 1.3350/0.4805
2023-03-29 18:08:24 | INFO | __main__:train:118) - Epoch: 009/010 | Batch 300/469 | Gen/Dis Loss: 1.7058/0.5018
2023-03-29 18:08:26 | INFO | __main__:train:118) - Epoch: 009/010 | Batch 400/469 | Gen/Dis Loss: 1.3628/0.5219
2023-03-29 18:08:27 | INFO | __main__:train:130) - Time elapsed: 1.23 min
2023-03-29 18:08:27 | INFO | __main__:train:118) - Epoch: 010/010 | Batch 000/469 | Gen/Dis Loss: 1.0598/0.5524
2023-03-29 18:08:29 | INFO | __main__:train:118) - Epoch: 010/010 | Batch 100/469 | Gen/Dis Loss: 0.8431/0.5962
2023-03-29 18:08:31 | INFO | __main__:train:118) - Epoch: 010/010 | Batch 200/469 | Gen/Dis Loss: 1.3003/0.5222
2023-03-29 18:08:32 | INFO | __main__:train:118) - Epoch: 010/010 | Batch 300/469 | Gen/Dis Loss: 0.7714/0.6205
2023-03-29 18:08:34 | INFO | __main__:train:118) - Epoch: 010/010 | Batch 400/469 | Gen/Dis Loss: 0.9008/0.5808
2023-03-29 18:08:35 | INFO | __main__:train:130) - Time elapsed: 1.36 min
2023-03-29 18:08:35 | INFO | __main__:train:132) - Total Training Time: 1.36 min
